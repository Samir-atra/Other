{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samir-atra/Other/blob/main/Capsa_experience/Capsa_Copy_of_Train%26Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6vfxK_0Ptlr"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import IPython\n",
        "import sys\n",
        "!pip install keras_tuner -q\n",
        "import keras_tuner\n",
        "!pip install git+https://github.com/themis-ai/capsa.git\n",
        "import capsa\n",
        "from capsa import EnsembleWrapper\n",
        "!pip install helper\n",
        "import helper\n",
        "\n",
        "from google.colab import drive             \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfp3guitPtlt"
      },
      "outputs": [],
      "source": [
        "# Dataset loading\n",
        "\n",
        "data_path = pathlib.Path('/content/drive/MyDrive/archiveX3/Training/')\n",
        "data_path_test = pathlib.Path('/content/drive/MyDrive/archiveX3/Testing/')\n",
        "\n",
        "dataset_path = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    labels= 'inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed= 1,\n",
        "    batch_size=64,\n",
        "    image_size=(64, 64),\n",
        "    color_mode=\"rgb\")\n",
        "    # shuffle=True)\n",
        "\n",
        "dataset_path_val = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    labels= 'inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed= 2,\n",
        "    batch_size=64,\n",
        "    image_size=(64, 64),\n",
        "    color_mode=\"rgb\")\n",
        "    # shuffle=True)\n",
        "\n",
        "dataset_path_test = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path_test,\n",
        "    labels= 'inferred',\n",
        "    seed= 3,\n",
        "    batch_size=64,\n",
        "    image_size=(64, 64),\n",
        "    color_mode=\"rgb\")\n",
        "    # shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZiBiEnNPtlu"
      },
      "outputs": [],
      "source": [
        "# # catch and prefetch\n",
        "# AUTOTUNE=tf.data.AUTOTUNE\n",
        "# dataset_path = dataset_path.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# dataset_path_val = dataset_path_val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1GdSSWvPtlw"
      },
      "outputs": [],
      "source": [
        "#classification cModel\n",
        "\n",
        "num_classes = 1\n",
        "\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.Input(shape=(64, 64, 3)),\n",
        "#     # tf.keras.layers.Rescaling(1./255),\n",
        "#     tf.keras.layers.Conv2D(16, 3, strides = 2, padding = \"same\",  activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),  \n",
        "#     tf.keras.layers.BatchNormalization(),\n",
        "#     # tf.keras.layers.Dropout(0.5),                                                                         #DP1\n",
        "#     tf.keras.layers.Conv2D(32, 3, strides = 2, padding = \"same\", activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "#     tf.keras.layers.BatchNormalization(),\n",
        "#     # tf.keras.layers.Dropout(0.5),                                                                         #DP2\n",
        "#     tf.keras.layers.Conv2D(64, 3, strides = 2, padding = \"same\", activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "#     tf.keras.layers.BatchNormalization(),\n",
        "#     # tf.keras.layers.Dropout(0.5),\n",
        "#     tf.keras.layers.Conv2D(128, 1, strides = 2, padding = \"same\", activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "#     tf.keras.layers.BatchNormalization(),\n",
        "#     # tf.keras.layers.Dropout(0.7),                                                                         #DP3\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(units = 512, activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "#     # tf.keras.layers.Dropout(0.2),                                                                         #DP4                                                                        #DP5\n",
        "#     tf.keras.layers.Dense(num_classes, activation= None)\n",
        "# ])\n",
        "\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
        "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "#     metrics=[tf.keras.metrics.BinaryAccuracy()], \n",
        "#     run_eagerly=True\n",
        "# )\n",
        "\n",
        "# model.fit(\n",
        "#         dataset_path,\n",
        "#         epochs=6,\n",
        "#         batch_size=64,\n",
        "#   )\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the HistogramVAEWrapper check\n",
        "\n",
        "# def build_model(hp):\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(64, 64, 3)),\n",
        "    # tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Conv2D(16, 3, strides = 2, padding = \"same\",  activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),  \n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # tf.keras.layers.Dropout(0.5),                                                                         #DP1\n",
        "    tf.keras.layers.Conv2D(32, 3, strides = 2, padding = \"same\", activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # tf.keras.layers.Dropout(0.5),                                                                         #DP2\n",
        "    tf.keras.layers.Conv2D(64, 3, strides = 2, padding = \"same\", activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Conv2D(128, 1, strides = 2, padding = \"same\", activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # tf.keras.layers.Dropout(0.7),                                                                         #DP3\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units = 512, activation='relu'),#, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    # tf.keras.layers.Dropout(0.2),                                                                         #DP4                                                                        #DP5\n",
        "    tf.keras.layers.Dense(num_classes, activation= None)\n",
        "])\n",
        "\n",
        "\n",
        "decoder = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=4*4*128),  \n",
        "        tf.keras.layers.Reshape((4, 4, 128)),\n",
        "        tf.keras.layers.Conv2DTranspose(64, 1,  strides=2, padding = \"same\", activation='relu'),\n",
        "        tf.keras.layers.Conv2DTranspose(32, 3,  strides=2, padding = \"same\", activation='relu'),\n",
        "        tf.keras.layers.Conv2DTranspose(16, 3,  strides=2, padding = \"same\"),\n",
        "        tf.keras.layers.Conv2DTranspose(3, 3,  strides=2, padding = \"same\"),\n",
        "    ])\n",
        "\n",
        "\n",
        "# learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-3, sampling=\"log\")\n",
        "\n",
        "wrapped_model = capsa.HistogramVAEWrapper(model, num_bins=5, queue_size=20000, latent_dim = 64, decoder=decoder)\n",
        "\n",
        "wrapped_model.compile(\n",
        "    optimizer= tf.keras.optimizers.Adam(learning_rate = 0.00001),\n",
        "    loss= tf.losses.BinaryCrossentropy(from_logits= True),\n",
        "    metrics= [tf.keras.metrics.BinaryAccuracy()]\n",
        ")\n",
        "\n",
        "#   return wrapped_model \n",
        "\n",
        "# build_model(keras_tuner.HyperParameters())\n",
        "\n",
        "# tuner = keras_tuner.RandomSearch(\n",
        "#     hypermodel=build_model,\n",
        "#     objective=\"vae_compiled_binary_accuracy\",\n",
        "#     max_trials=5,\n",
        "#     executions_per_trial=2,\n",
        "#     overwrite=True,\n",
        "#     directory=\"/content/drive/MyDrive/Tuner_Capsa\",\n",
        "#     project_name=\"Tuner\",\n",
        "# )\n",
        "\n",
        "# tuner.search_space_summary()\n",
        "\n",
        "# tuner.search(dataset_path, epochs=5, validation_data = dataset_path_val)\n",
        "\n",
        "# tuner.results_summary()\n",
        "\n",
        "# wrapped_model.summary()\n",
        "\n",
        "history = wrapped_model.fit(\n",
        "    dataset_path,\n",
        "    epochs=10,                                    \n",
        "    validation_data = dataset_path_val)\n",
        "\n",
        "\n",
        "out = wrapped_model.predict(dataset_path_test, batch_size=512)\n"
      ],
      "metadata": {
        "id": "HqkZqKoNbKMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mve_model = capsa.MVEWrapper(model, is_classification=True)\n",
        "\n",
        "mve_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()], \n",
        "    run_eagerly=True\n",
        ")\n",
        "\n",
        "history = mve_model.fit(\n",
        "        dataset_path,\n",
        "        epochs=6,\n",
        "        batch_size=64,\n",
        "  )\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "normalized_train_ds = dataset_path.map(lambda x, y: (normalization_layer(x), y))\n",
        "normalized_test_ds = dataset_path_test.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "images = np.vstack((normalized_test_ds, normalized_train_ds))\n",
        "\n",
        "# print(type(images[1][0]))\n",
        "\n",
        "# images= tf.convert_to_tensor(np.asarray(images).astype('float32'))\n",
        "\n",
        "# predictions = model(images)"
      ],
      "metadata": {
        "id": "Pq5nyLc-JMYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_wrapped_model = EnsembleWrapper(model,num_members=1)\n",
        "\n",
        "\n",
        "Ensemble_wrapped_model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "                     )\n",
        "\n",
        "history = Ensemble_wrapped_model.fit(\n",
        "        dataset_path,\n",
        "        epochs=12,\n",
        "        batch_size=64,\n",
        "  )\n",
        "\n",
        "# mesh_grid = helper.get_grid()\n",
        "# output = Ensemble_wrapped_model(mesh_grid)\n",
        "# helper.plot_ensemble_classification(output,mesh_grid,dataset_path_test)"
      ],
      "metadata": {
        "id": "hmGa4UoEwuwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from capsa import DropoutWrapper\n",
        "\n",
        "base_model = unet(drop_prob=0.1)\n",
        "# don't add dropout in the wrapper because our\n",
        "# model already contains dropout layers\n",
        "model = DropoutWrapper(base_model, p=0.0)"
      ],
      "metadata": {
        "id": "eoOywcJC4Oka"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d4de0671d8054c88a8842951ed8f6115b974e2e5b39cbc2553be5a6678282e63"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}